\setchapterstyle{plain}
\pagelayout{wide}

\chapter{Code source de la collecte et de l'analyse des données}

\section{Collecte initiale depuis le graphe de Software Heritage}
\label{app:collect.java}

Le code de la collecte initiale prend la forme d'une classe Java utilisant la bibliothèque
\texttt{swh-graph}\footnote{\url{https://docs.softwareheritage.org/devel/swh-graph/java-api.html}} et est
disponible sur le dépôt \gls{github} de ce mémoire au lien suivant :
\url{https://github.com/Dettorer/synva-dissertation/blob/main/experiment/data_collection/CollectData.java}.

La version de \texttt{swh-graph} utilisée est cependant une version modifiée pour rendre la plupart des
fonctions d'accès aux données du graphe "\en{thread safe}", c'est à dire utilisables dans un contexte
parallélisé sans besoin de synchronisation des fils d'exécution pour éviter les appels concurrents. Cette
version de la bibliothèque permet l'implémentation d'algorithmes parallélisés beaucoup plus rapides, car
utilisant beaucoup moins de points de synchronisation.

Dans un premier temps, la fonction \texttt{discoverProject} est appelée sur tous les nœuds de type
\texttt{ORI} (point de départ de l'archivage d'un projet par Software Heritage), cette fonction identifie tous
les projets étant des \gls{fork} de celui-ci via un double parcours largeur sur la composante connexe du nœud
de départ formée par le sous-graphe des nœuds \texttt{REV} (\en{revision}, terme générique pour les
\glspl{commit}) et \texttt{SNP} (\en{snapshot}, le point d'entrée d'\emph{un} archivage du projet). Cette
détection se fait en deux parcours largeur au lieu d'un seul afin de calculer en même temps la taille de la
plus longue chaîne de commits accessible depuis chaque nœud \texttt{ORI} de la composante connexe (donc depuis
chaque \gls{fork} du projet initial). Le premier parcours remonte les ancêtres du nœud de départ pour trouver
les révisions racines (les "\en{initial \glspl{commit}}") du projet, puis un deuxième parcours est lancé dans
l'autre sens avec des marqueurs de niveau depuis chacune de ces révisions racines afin de trouver tous les
nœuds \texttt{ORI} qui peuvent les atteindre et sont donc des \glspl{fork} les uns des autres. Pour chaque
composante connexe, seul un nœud \texttt{SNP} est retenu pour l'analyse en deuxième étape : celui étant le
point de départ de la plus longue chaîne de \glspl{commit} possible, donc ayant le plus de données
exploitables.

Dans un deuxième temps, la fonction \texttt{collectProject} est appelée sur chaque projet retenu afin d'en
extraire les données de recherche. Cette fonction commence par identifier la branche ayant le plus de chance
d'être la branche principale du projet (voir la table de priorité \texttt{mainBranchScore}), puis démarre un
parcours largeur à partir de cette branche dans lequel elle compte :

\begin{itemize}
    \item le nombre de contributeurs pendant la période de référence n'ayant jamais contribué dans ce projet
        avant (variable expliquée) ;
    \item le nombre de contributeurs uniques pendant une période récente avant la période de référence
        (variable explicative, hypothèse H\ref{hyp:recentcontributorcount}) ;
    \item le nombre de \glspl{commit} dans la même période récente avant la période de référence (variable
        explicative, hypothèse H\ref{hyp:recentcommitcount}).
\end{itemize}

La présence d'instructions de contribution (hypothèse H\ref{hyp:contributionguidelines}) est plus compliquée à
vérifier. Le graphe possédant le nom et la hiérarchie des fichiers disponibles à chaque \gls{commit}, mais pas
leur contenu, l'analyse se contente de vérifier si un fichier dont le nom est une variante du classique
\texttt{CONTRIBUTING.md} existe. Si ce fichier existe, l'analyse conclue que le projet possède effectivement
des instructions de contribution, sinon, elle vérifie la présence d'un fichier dont le nom est une variante du
classique \texttt{README.md}. Si ce fichier existe, l'analyse conclue que le projet possède \emph{peut être}
des instructions de contribution et construit l'URL à laquelle un traitement ultérieur pourra télécharger le
contenu du fichier \texttt{README} et y confirmer ou non la présence d'instructions de contribution. Si aucun
de ces deux types de fichier n'existe, l'analyse conclue que le projet ne possède pas d'instructions de
contribution.

Enfin, les données collectées sont affichées sur la sortie standard sous la forme d'un fichier CSV.

\section{Collecte complémentaire : analyse des fichiers \texttt{README}}
\label{app:checkreadme.py}

Le code de cette collecte complémentaire prend la forme d'un script Python utilisant plusieurs bibliothèques
comme \texttt{requests} pour les requêtes sur \url{https://archive.softwareheritage.org/}, \texttt{boto3} pour
les requêtes Amazon S3, ou \texttt{charset\_normalizer} pour le décodage des fichiers brut. Le script complet
est disponible sur le dépôt \gls{github} de ce mémoire au lien suivant :
\url{https://github.com/Dettorer/synva-dissertation/blob/main/experiment/data_collection/check_readme_contents.py}.

Une difficulté de la récolte du contenu des fichiers \texttt{README} est une incohérence dans le type
d'identificateur utilisé sur \url{https://archive.softwareheritage.org/} et le \en{registry} Amazon S3. Pour
télécharger le contenu d'un fichier depuis le site web, il faut l'identifier en utilisant son hachage
cryptographique \texttt{sha1\_git}, qui est celui renseigné dans le graphe. Le \en{registry} Amazon S3, en
revanche, identifie les contenus en utilisant un hachage différent appelé \texttt{sha1}. Pour pouvoir
télécharger le contenu d'un fichier sur Amazon S3 à partir de données collectées dans le graphe, il faut donc
traduire le \texttt{sha1\_git} obtenu en un \texttt{sha1}, ce qui est impossible si l'on ne possède pour seule
information que le \texttt{sha1\_git}. Pour contourner le problème, une table de correspondance des deux
méthodes de hachage peut être construite préalablement à partir de la représentation Apache ORC du
graph\footnote{voir \url{https://docs.softwareheritage.org/devel/swh-dataset/graph/dataset.html}}, qui
contient les deux types de hash.

\section{Code d'analyse des résultats}
\label{app:analysis.py}

Le code utilisé pour produire les analyses du chapitre \ref{chap:results} prend la forme d'un script Python
utilisant plusieurs bibliothèques de traitement des données et d'analyse statistique comme \texttt{pandas},
\texttt{numpy}, \texttt{scipy}, \texttt{statsmodels}, ainsi que \texttt{matplotlib} pour la production des
visualisations. Le script complet est disponible sur le dépôt \gls{github} de ce mémoire au lien suivant :
\url{https://github.com/Dettorer/synva-dissertation/blob/main/experiment/data_analysis/data_analysis.py}.

\todo[inline]{replication package ?}
